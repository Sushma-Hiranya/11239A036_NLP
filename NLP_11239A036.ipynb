{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization**"
      ],
      "metadata": {
        "id": "buPSSxVDswAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "print(\"Lemmatized word:\", WordNetLemmatizer().lemmatize(input(\"Enter a word: \")))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq8UJBqHlGqN",
        "outputId": "429ba2c2-d993-4f29-cd06-34a8d767814c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a word: bank\n",
            "Lemmatized word: bank\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalization**"
      ],
      "metadata": {
        "id": "AceuBUEZs3UU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "text = \"Hello World!! This is Python\"\n",
        "print(\"Normalized text:\", normalize_text(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3aL_nQ-rcX_",
        "outputId": "01ce76ed-3372-4c08-c53a-7f9fe8928744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized text: hello world!! this is python\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "rDhF4u5hs6LJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello World This is simple tokenization example\"\n",
        "tokens = text.split()\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpNW_037rztF",
        "outputId": "a699c66c-7c15-44a1-84ce-76b5a6e7bb76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'World', 'This', 'is', 'simple', 'tokenization', 'example']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming**"
      ],
      "metadata": {
        "id": "H8zD2xN6s_ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import warnings\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "text = \"Cars running faster than the other animals\"\n",
        "words = word_tokenize(text)\n",
        "\n",
        "ps = PorterStemmer()\n",
        "stemmed = [ps.stem(w) for w in words]\n",
        "\n",
        "print(stemmed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebsxFcoIr5dy",
        "outputId": "fdaa1a56-861f-4b7c-96db-d921f48c3a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['car', 'run', 'faster', 'than', 'the', 'other', 'anim']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Morphology**"
      ],
      "metadata": {
        "id": "8UoIYUDDtDNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"The cats are running quickly\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, \"→\", token.lemma_, \"|\", token.pos_, \"|\", token.morph)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIE27_xUsOWG",
        "outputId": "04d47a53-0c5a-4299-b90d-bc4b224d2a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The → the | DET | Definite=Def|PronType=Art\n",
            "cats → cat | NOUN | Number=Plur\n",
            "are → be | AUX | Mood=Ind|Tense=Pres|VerbForm=Fin\n",
            "running → run | VERB | Aspect=Prog|Tense=Pres|VerbForm=Part\n",
            "quickly → quickly | ADV | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spelling Correction**"
      ],
      "metadata": {
        "id": "0GWPWM5BtLVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install textblob -q\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "text = \"I lik to lern naturall langauge procesing\"\n",
        "blob = TextBlob(text)\n",
        "\n",
        "print(blob.correct())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlvPk3N5scTf",
        "outputId": "d868ad70-7388-4dac-ccf4-8747bd4757ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I like to learn natural language processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deduction**"
      ],
      "metadata": {
        "id": "g8EkNB6NtOUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sem import Expression\n",
        "from nltk.inference import ResolutionProver\n",
        "\n",
        "read_expr = Expression.fromstring\n",
        "\n",
        "kb = [\n",
        "    read_expr('man(Socrates)'),\n",
        "    read_expr('all x (man(x) -> mortal(x))')\n",
        "]\n",
        "\n",
        "goal = read_expr('mortal(Socrates)')\n",
        "\n",
        "print(ResolutionProver().prove(goal, kb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z74ssgusmZn",
        "outputId": "bb9f29aa-a272-47f8-dc19-6d5f55a95a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unigram**"
      ],
      "metadata": {
        "id": "3mNEl20Pn5zh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Yp5X3_fnjOz",
        "outputId": "3129c744-e121-44e7-83e6-ff2f8e8381bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'I': 2, 'love': 2, 'natural': 1, 'language': 1, 'processing': 1, 'and': 1, 'coding': 1}\n"
          ]
        }
      ],
      "source": [
        "text = \"I love natural language processing and I love coding\"\n",
        "\n",
        "words = text.split()\n",
        "\n",
        "freq = {}\n",
        "for word in words:\n",
        "    freq[word] = freq.get(word, 0) + 1\n",
        "\n",
        "print(freq)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bigram**"
      ],
      "metadata": {
        "id": "QPoMQ9Q2oGsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I love natural language processing and I love coding\"\n",
        "\n",
        "words = text.split()\n",
        "\n",
        "bigrams = []\n",
        "for i in range(len(words) - 1):\n",
        "    bigrams.append((words[i], words[i+1]))\n",
        "\n",
        "freq = {}\n",
        "for bigram in bigrams:\n",
        "    freq[bigram] = freq.get(bigram, 0) + 1\n",
        "\n",
        "print(freq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JFnJVS_ntGt",
        "outputId": "41be77dc-ed28-447b-d4dc-e896e364ab89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('I', 'love'): 2, ('love', 'natural'): 1, ('natural', 'language'): 1, ('language', 'processing'): 1, ('processing', 'and'): 1, ('and', 'I'): 1, ('love', 'coding'): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trigram**"
      ],
      "metadata": {
        "id": "OcEfgAdHoKCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I love natural language processing and I love coding\"\n",
        "\n",
        "words = text.split()\n",
        "\n",
        "trigrams = []\n",
        "for i in range(len(words) - 2):\n",
        "    trigrams.append((words[i], words[i+1], words[i+2]))\n",
        "\n",
        "freq = {}\n",
        "for trigram in trigrams:\n",
        "    freq[trigram] = freq.get(trigram, 0) + 1\n",
        "\n",
        "print(freq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy3qTDP6nws5",
        "outputId": "cf44e3a6-05c7-475a-f547-56d0127e4bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('I', 'love', 'natural'): 1, ('love', 'natural', 'language'): 1, ('natural', 'language', 'processing'): 1, ('language', 'processing', 'and'): 1, ('processing', 'and', 'I'): 1, ('and', 'I', 'love'): 1, ('I', 'love', 'coding'): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N-gram Smoothing**"
      ],
      "metadata": {
        "id": "CiIjqlM-oM8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "text = \"I love NLP I love machine learning\"\n",
        "words = text.split()\n",
        "V = len(set(words))\n",
        "\n",
        "unigrams = Counter(words)\n",
        "bigrams = Counter([(words[i], words[i+1]) for i in range(len(words)-1)])\n",
        "\n",
        "def laplace_prob(w1, w2):\n",
        "    return (bigrams[(w1, w2)] + 1) / (unigrams[w1] + V)\n",
        "\n",
        "print(\"P(love | I) =\", laplace_prob(\"I\", \"love\"))\n",
        "print(\"P(NLP | love) =\", laplace_prob(\"love\", \"NLP\"))\n",
        "print(\"P(machine | NLP) =\", laplace_prob(\"NLP\", \"machine\"))\n",
        "print(\"P(learning | machine) =\", laplace_prob(\"machine\", \"learning\"))\n",
        "print(\"P(unknown | NLP) =\", laplace_prob(\"NLP\", \"unknown\"))  # unseen word\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYE8KB_cn2a3",
        "outputId": "c135ca47-929b-4fec-b8e8-7adb0effdce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(love | I) = 0.42857142857142855\n",
            "P(NLP | love) = 0.2857142857142857\n",
            "P(machine | NLP) = 0.16666666666666666\n",
            "P(learning | machine) = 0.3333333333333333\n",
            "P(unknown | NLP) = 0.16666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POS Tagging**"
      ],
      "metadata": {
        "id": "OnRqOvSgpttp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "\n",
        "text = \"I love learning NLP\"\n",
        "words = nltk.word_tokenize(text)\n",
        "print(nltk.pos_tag(words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMORqyfHpDh0",
        "outputId": "877a14bd-ca1c-45e9-82e7-b43f00ceab41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('love', 'VBP'), ('learning', 'VBG'), ('NLP', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HMM**"
      ],
      "metadata": {
        "id": "IZSe6drnqKrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tag import hmm\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "train_data = [[\n",
        "    ('I', 'PRONOUN'),\n",
        "    ('love', 'VERB'),\n",
        "    ('dogs', 'NOUN')\n",
        "], [\n",
        "    ('You', 'PRONOUN'),\n",
        "    ('love', 'VERB'),\n",
        "    ('cats', 'NOUN')\n",
        "]]\n",
        "\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "tagger = trainer.train_supervised(train_data)\n",
        "\n",
        "sentence = ['I', 'love', 'cats']\n",
        "print(tagger.tag(sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbAhMQ9xpr8A",
        "outputId": "8c8b2786-fa8b-4a2c-c3a7-f5d1783929e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRONOUN'), ('love', 'VERB'), ('cats', 'NOUN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Brill POS Tagger**"
      ],
      "metadata": {
        "id": "j0vGH7rKrDv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tag import brill, brill_trainer, UnigramTagger\n",
        "\n",
        "nltk.download('treebank')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.treebank.tagged_sents(tagset='universal')[:3000]\n",
        "uni = UnigramTagger(data)\n",
        "tagger = brill_trainer.BrillTaggerTrainer(uni, brill.fntbl37()).train(data)\n",
        "\n",
        "print(tagger.tag(\"I love learning NLP\".split()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZRXv8q80m3X",
        "outputId": "043606ec-fabd-4f74-c017-d44bb0730f18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRON'), ('love', None), ('learning', 'NOUN'), ('NLP', None)]\n"
          ]
        }
      ]
    }
  ]
}